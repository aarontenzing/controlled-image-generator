{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aaron\\Documents\\git\\controlnet\\.myvenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel\n",
    "from diffusers import EulerDiscreteScheduler # Euler Discrete Scheduler\n",
    "from diffusers import DPMSolverMultistepScheduler # Import samplers\n",
    "from diffusers import DDIMScheduler # Import samplers\n",
    "from diffusers.utils import load_image\n",
    "from tqdm import tqdm\n",
    "from prompt import prompt_generator, prompt_generator_no_human # Prompt generation\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Samplers (schedulers): https://huggingface.co/docs/diffusers/api/schedulers/overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.2.2+cu121\n",
      "Is CUDA enabled? True\n"
     ]
    }
   ],
   "source": [
    "print(\"Torch version:\",torch.__version__)\n",
    "print(\"Is CUDA enabled?\",torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://thepythoncode.com/article/control-generated-images-with-controlnet-with-huggingface#code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Pipeline: ControlNet + Stable Diffusion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unet\\diffusion_pytorch_model.safetensors not found\n",
      "Loading pipeline components...: 100%|██████████| 6/6 [00:10<00:00,  1.82s/it]\n",
      "You have disabled the safety checker for <class 'diffusers.pipelines.controlnet.pipeline_controlnet.StableDiffusionControlNetPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n"
     ]
    }
   ],
   "source": [
    "# ControlNet model\n",
    "controlnet = ControlNetModel.from_pretrained(\"lllyasviel/control_v11p_sd15_scribble\", torch_dtype=torch.float16)\n",
    "\n",
    "# Define stable diffusion pipeline with controlnet\n",
    "# We use the realistic-vision-v20-2047 model for this example (finetunned on realistic images of people)\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\"stablediffusionapi/realistic-vision-v20-2047\", controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)\n",
    "DPM = DPMSolverMultistepScheduler(use_karras_sigmas=True)\n",
    "pipe.scheduler = DPM.from_config(pipe.scheduler.config)\n",
    "\n",
    "# Enable efficient implementations using xformers for faster inference\n",
    "pipe.enable_xformers_memory_efficient_attention()\n",
    "pipe.enable_model_cpu_offload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: A medium shot view of an old man holding a brown cardboard box with black tape from the Zara webshop in his hands, standing in front of a quite neighborhood street and cars background, the weather is rainy, high photorealistic quality.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:09<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save test.jpg!\n"
     ]
    }
   ],
   "source": [
    "# Load wireframe image\n",
    "image_input = load_image(\"wireframes\\\\3_4.jpg\")\n",
    "\n",
    "# Prompt\n",
    "prompt = prompt_generator()\n",
    "print(\"Prompt:\",prompt)\n",
    "neg_prompt = \"disfigured, not realistic, low quality\"\n",
    "# Run the pipeline\n",
    "image_output = pipe(prompt=prompt, negative_prompt=neg_prompt, image=image_input, num_inference_steps=18).images[0]\n",
    "\n",
    "# Save the output\n",
    "image_output.save(\"test.jpg\")\n",
    "print(\"Save test.jpg!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating first synthetic dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"wireframes\" are drawn in the bottom half region of the window, so they can easily be held  by persons.\n",
    "Next, we will generate synthetic images of those wireframes, located in the directory \"outputs\".\n",
    "- I generate random prompts\n",
    "- Next I will validate them\n",
    "- Finally, I will only keep the images with a single box in the image (classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ControlNet model: lllyasviel/control_v11p_sd15_scribble\"\n",
    "Stable diffusion model: stablediffusionapi/realistic-vision-v20-2047"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images(controlnet_model_path, sd_model_path, prompt_human, infer_steps):\n",
    "        \n",
    "    # ControlNet model\n",
    "    controlnet = ControlNetModel.from_pretrained(controlnet_model_path, torch_dtype=torch.float16)\n",
    "\n",
    "    # Define stable diffusion pipeline with controlnet\n",
    "    # We use the realistic-vision-v20-2047 model for this example (finetunned on realistic images of people)\n",
    "    pipe = StableDiffusionControlNetPipeline.from_pretrained(sd_model_path, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)\n",
    "    \n",
    "    DPM = DPMSolverMultistepScheduler(use_karras_sigmas=True) # DPM++ 2M Karras\n",
    "    pipe.scheduler = DPM.from_config(pipe.scheduler.config)\n",
    "\n",
    "    # Enable efficient implementations using xformers for faster inference\n",
    "    pipe.enable_xformers_memory_efficient_attention()\n",
    "    pipe.enable_model_cpu_offload()\n",
    "    \n",
    "    # Read the files in the directory\n",
    "    files = os.listdir(\"wireframes\")\n",
    "    sorted_files = sorted(files, key=lambda x: (int(x.split('_')[0]), int(x.split('_')[1].split('.')[0]))) \n",
    "\n",
    "    for file in tqdm(sorted_files):\n",
    "        image_input = load_image(\"wireframes\\\\\" + file)\n",
    "        prompt = prompt_generator_no_human() if prompt_human == 0 else prompt_generator()\n",
    "        # print(prompt)\n",
    "        image_output = pipe(prompt=prompt, negative_prompt=\"flat background, person, human, disfigured, unrealistic, low quality\", image=image_input, num_inference_steps = infer_steps).images[0]\n",
    "        image_output.save(os.path.join(\"outputs\\\\\", os.path.basename(file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vae\\diffusion_pytorch_model.safetensors not found\n",
      "Loading pipeline components...: 100%|██████████| 6/6 [00:05<00:00,  1.17it/s]\n",
      "You have disabled the safety checker for <class 'diffusers.pipelines.controlnet.pipeline_controlnet.StableDiffusionControlNetPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n",
      "100%|██████████| 15/15 [00:08<00:00,  1.74it/s]\n",
      "100%|██████████| 15/15 [00:08<00:00,  1.67it/s]it]\n",
      "100%|██████████| 15/15 [00:08<00:00,  1.67it/s]it]\n",
      "100%|██████████| 15/15 [00:08<00:00,  1.69it/s]it]\n",
      "100%|██████████| 15/15 [00:08<00:00,  1.67it/s]it]\n",
      "100%|██████████| 15/15 [00:09<00:00,  1.63it/s]it]\n",
      "100%|██████████| 15/15 [00:07<00:00,  1.94it/s]it]\n",
      "  0%|          | 7/2000 [01:23<6:28:28, 11.70s/it]"
     ]
    }
   ],
   "source": [
    "generate_images(\"lllyasviel/control_v11p_sd15_scribble\", \"stablediffusionapi/realistic-vision-v20-2047\", 0, infer_steps=15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
