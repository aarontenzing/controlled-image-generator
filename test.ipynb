{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel\n",
    "from diffusers import EulerDiscreteScheduler # Import samplers\n",
    "from diffusers import DPMSolverMultistepScheduler # Import samplers\n",
    "from diffusers.utils import load_image\n",
    "from tqdm import tqdm\n",
    "from prompt import prompt_generator, prompt_generator_no_human"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "runwayml/stable-diffusion-v1-5\n",
    "stablediffusionapi/realistic-vision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unet\\diffusion_pytorch_model.safetensors not found\n",
      "Loading pipeline components...:  33%|███▎      | 2/6 [00:07<00:15,  3.98s/it]c:\\Users\\aaron\\Documents\\git\\controlnet\\.myvenv\\Lib\\site-packages\\transformers\\models\\clip\\feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n",
      "Loading pipeline components...: 100%|██████████| 6/6 [00:12<00:00,  2.10s/it]\n",
      "You have disabled the safety checker for <class 'diffusers.pipelines.controlnet.pipeline_controlnet.StableDiffusionControlNetPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n"
     ]
    }
   ],
   "source": [
    "# ControlNet model\n",
    "controlnet = ControlNetModel.from_pretrained(\"lllyasviel/control_v11p_sd15_scribble\", torch_dtype=torch.float16)\n",
    "\n",
    "# Define stable diffusion pipeline with controlnet\n",
    "# We use the realistic-vision-v20-2047 model for this example (finetunned on realistic images of people)\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\"stablediffusionapi/realistic-vision-v20-2047\", controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)\n",
    "pipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "# Enable efficient implementations using xformers for faster inference\n",
    "pipe.enable_xformers_memory_efficient_attention()\n",
    "pipe.enable_model_cpu_offload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A close-up view of a colorful cardboard box with a barcode from the Bol.com webshop, in front of a quite neighborhood street and garden background, the weather is snowy, high photorealistic quality.\n"
     ]
    }
   ],
   "source": [
    "prompt = prompt_generator_no_human()\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:16<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save test.jpg!\n"
     ]
    }
   ],
   "source": [
    "# Load wireframe image\n",
    "image_input = load_image(\"wireframes_set1\\\\img5_1.jpg\")\n",
    "\n",
    "# Prompt\n",
    "\n",
    "prompt = prompt_generator_no_human()\n",
    "\n",
    "# Run the pipeline\n",
    "image_output = pipe(prompt=prompt, negative_prompt=\"poor quality, cartoon, mulitiple boxes, cardboard in the background\", image=image_input, num_inference_steps=30).images[0]\n",
    "\n",
    "# Save the output\n",
    "image_output.save(f\"test.jpg\")\n",
    "print(\"Save test.jpg!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
